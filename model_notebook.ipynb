{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tinystorycustom.model')\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "train_data = dataset[\"train\"][\"text\"]\n",
    "\n",
    "max_length = 0\n",
    "for passage_list in train_data:\n",
    "        # Assume `tokenizer.encode` returns a list of tokens.\n",
    "        encoded_paragraph = sp.encode_as_ids(passage_list)\n",
    "        max_length = max(max_length, len(encoded_paragraph))\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.IdToPiece(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.PieceToId('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "pad_token_id = sp.PieceToId('<pad>')\n",
    "eos_token_id = sp.PieceToId('</s>')\n",
    "print(eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(text, max_length, pad_token_id):\n",
    "    encoded_text = sp.encode_as_ids(text)\n",
    "    padded_text = encoded_text + [eos_token_id] +[pad_token_id] * (max_length - len(encoded_text)),  # Pad at the end\n",
    "        \n",
    "    return padded_text\n",
    "\n",
    "padded_stories = [encode_and_pad(text, max_length, pad_token_id) for text in dataset[\"train\"][\"text\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * input_dim)\n",
    "        self.fc_out = nn.Linear(input_dim, input_dim)\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, input_dim),\n",
    "        )\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        d_k = q.size(-1)\n",
    "        attn_logits = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        attention = torch.softmax(attn_logits, dim=-1)\n",
    "        values = torch.matmul(attention, v)\n",
    "        return values\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, dim = x.size()\n",
    "        \n",
    "        # Linear projection splits Q, K, and V, and then reshape and transpose for multi-head attention\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        q = q.view(batch_size, seq_length, self.num_heads, dim // self.num_heads).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_length, self.num_heads, dim // self.num_heads).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_length, self.num_heads, dim // self.num_heads).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn = self.scaled_dot_product_attention(q, k, v)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(batch_size, seq_length, dim)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Add & Norm\n",
    "        x = x + attn\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ff_out = self.feed_forward(x)\n",
    "        ff_out = self.dropout(ff_out)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = x + ff_out\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, input_dim, num_heads, dim_feedforward, num_layers, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, input_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 512, input_dim))\n",
    "        self.layers = nn.ModuleList([TransformerBlock(input_dim, num_heads, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(input_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.embed(src) * math.sqrt(self.embed.embedding_dim)\n",
    "        src = src + self.pos_encoder[:,:src.size(1)]\n",
    "        src = nn.Dropout(0.1)(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "        \n",
    "        output = self.fc_out(src)\n",
    "        return output\n",
    "\n",
    "# Example parameters:\n",
    "vocab_size = 16000  # Vocabulary size\n",
    "input_dim = 512  # Embedding dimension\n",
    "num_heads = 8  # Number of heads in multi-head attention\n",
    "dim_feedforward = 2048  # Hidden layer size in feed forward network\n",
    "num_layers = 1  # Number of transformer blocks\n",
    "\n",
    "# Initialize the model\n",
    "transformer_model = TransformerModel(vocab_size, input_dim, num_heads, dim_feedforward, num_layers)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example data (token IDs)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, data_loader, epochs, criterion, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch, tgt in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Shift the target sequence by one for next word prediction\n",
    "            target_input = tgt[:, :-1]\n",
    "            targets = tgt[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            output = transformer_model(target_input)\n",
    "            output = output.view(-1, transformer_model.fc_out.out_features)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "input_sequences = padded_stories\n",
    "target_sequences = torch.roll(input_sequences, shifts=-1, dims=1)\n",
    "\n",
    "# Create dataset and data loader\n",
    "dataset = TensorDataset(input_sequences, target_sequences)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer_model.parameters())\n",
    "\n",
    "epochs = 5  # Number of epochs to train\n",
    "train_model(transformer_model, data_loader, epochs, criterion, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
